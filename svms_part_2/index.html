<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <meta name="generator" content="Hugo 0.72.0" />
  <link rel="canonical" href="https://example.com/svms_part_2/">

  

  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#000000">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="theme-color" content="#ffffff">

  <link rel="stylesheet" href="https://example.com/css/prism.css" media="none" onload="this.media='all';">

  
  
  <link rel="stylesheet" type="text/css" href="https://example.com/css/styles.css">

  

  <style id="inverter" media="none">
    .intro-and-nav, .main-and-footer { filter: invert(100%) }
    * { background-color: inherit }
    img:not([src*=".svg"]), .colors, iframe, .demo-container { filter: invert(100%) }
  </style>

  
  
  <title>SVMs - Part II (Application) | Cupper</title>
</head>

  <body>
    <a href="#main">skip to content</a>
    <svg style="display: none">
  <symbol id="bookmark" viewBox="0 0 40 50">
   <g transform="translate(2266 3206.2)">
    <path style="stroke:currentColor;stroke-width:3.2637;fill:none" d="m-2262.2-3203.4-.2331 42.195 16.319-16.318 16.318 16.318.2331-42.428z"/>
   </g>
  </symbol>

  <symbol id="w3c" viewBox="0 0 127.09899 67.763">
   <text font-size="83" style="font-size:83px;font-family:Trebuchet;letter-spacing:-12;fill-opacity:0" letter-spacing="-12" y="67.609352" x="-26.782778">W3C</text>
   <text font-size="83" style="font-size:83px;font-weight:bold;font-family:Trebuchet;fill-opacity:0" y="67.609352" x="153.21722" font-weight="bold">SVG</text>
   <path style="fill:currentColor;image-rendering:optimizeQuality;shape-rendering:geometricPrecision" d="m33.695.377 12.062 41.016 12.067-41.016h8.731l-19.968 67.386h-.831l-12.48-41.759-12.479 41.759h-.832l-19.965-67.386h8.736l12.061 41.016 8.154-27.618-3.993-13.397h8.737z"/>
   <path style="fill:currentColor;image-rendering:optimizeQuality;shape-rendering:geometricPrecision" d="m91.355 46.132c0 6.104-1.624 11.234-4.862 15.394-3.248 4.158-7.45 6.237-12.607 6.237-3.882 0-7.263-1.238-10.148-3.702-2.885-2.47-5.02-5.812-6.406-10.022l6.82-2.829c1.001 2.552 2.317 4.562 3.953 6.028 1.636 1.469 3.56 2.207 5.781 2.207 2.329 0 4.3-1.306 5.909-3.911 1.609-2.606 2.411-5.738 2.411-9.401 0-4.049-.861-7.179-2.582-9.399-1.995-2.604-5.129-3.912-9.397-3.912h-3.327v-3.991l11.646-20.133h-14.062l-3.911 6.655h-2.493v-14.976h32.441v4.075l-12.31 21.217c4.324 1.385 7.596 3.911 9.815 7.571 2.22 3.659 3.329 7.953 3.329 12.892z"/>
   <path style="fill:currentColor;image-rendering:optimizeQuality;shape-rendering:geometricPrecision" d="m125.21 0 1.414 8.6-5.008 9.583s-1.924-4.064-5.117-6.314c-2.693-1.899-4.447-2.309-7.186-1.746-3.527.73-7.516 4.938-9.258 10.13-2.084 6.21-2.104 9.218-2.178 11.978-.115 4.428.58 7.043.58 7.043s-3.04-5.626-3.011-13.866c.018-5.882.947-11.218 3.666-16.479 2.404-4.627 5.954-7.404 9.114-7.728 3.264-.343 5.848 1.229 7.841 2.938 2.089 1.788 4.213 5.698 4.213 5.698l4.94-9.837z"/>
   <path style="fill:currentColor;image-rendering:optimizeQuality;shape-rendering:geometricPrecision" d="m125.82 48.674s-2.208 3.957-3.589 5.48c-1.379 1.524-3.849 4.209-6.896 5.555-3.049 1.343-4.646 1.598-7.661 1.306-3.01-.29-5.807-2.032-6.786-2.764-.979-.722-3.486-2.864-4.897-4.854-1.42-2-3.634-5.995-3.634-5.995s1.233 4.001 2.007 5.699c.442.977 1.81 3.965 3.749 6.572 1.805 2.425 5.315 6.604 10.652 7.545 5.336.945 9.002-1.449 9.907-2.031.907-.578 2.819-2.178 4.032-3.475 1.264-1.351 2.459-3.079 3.116-4.108.487-.758 1.276-2.286 1.276-2.286l-1.276-6.644z"/>
  </symbol>

  <symbol id="tag" viewBox="0 0 177.16535 177.16535">
    <g transform="translate(0 -875.2)">
     <path style="fill-rule:evenodd;stroke-width:0;fill:currentColor" d="m159.9 894.3-68.79 8.5872-75.42 77.336 61.931 60.397 75.429-76.565 6.8495-69.755zm-31.412 31.835a10.813 10.813 0 0 1 1.8443 2.247 10.813 10.813 0 0 1 -3.5174 14.872l-.0445.0275a10.813 10.813 0 0 1 -14.86 -3.5714 10.813 10.813 0 0 1 3.5563 -14.863 10.813 10.813 0 0 1 13.022 1.2884z"/>
    </g>
  </symbol>

  <symbol id="balloon" viewBox="0 0 141.73228 177.16535">
   <g transform="translate(0 -875.2)">
    <g>
     <path style="fill:currentColor" d="m68.156 882.83-.88753 1.4269c-4.9564 7.9666-6.3764 17.321-5.6731 37.378.36584 10.437 1.1246 23.51 1.6874 29.062.38895 3.8372 3.8278 32.454 4.6105 38.459 4.6694-.24176 9.2946.2879 14.377 1.481 1.2359-3.2937 5.2496-13.088 8.886-21.623 6.249-14.668 8.4128-21.264 10.253-31.252 1.2464-6.7626 1.6341-12.156 1.4204-19.764-.36325-12.93-2.1234-19.487-6.9377-25.843-2.0833-2.7507-6.9865-7.6112-7.9127-7.8436-.79716-.20019-6.6946-1.0922-6.7755-1.0248-.02213.0182-5.0006-.41858-7.5248-.22808l-2.149-.22808h-3.3738z"/>
     <path style="fill:currentColor" d="m61.915 883.28-3.2484.4497c-1.7863.24724-3.5182.53481-3.8494.63994-2.4751.33811-4.7267.86957-6.7777 1.5696-.28598 0-1.0254.20146-2.3695.58589-5.0418 1.4418-6.6374 2.2604-8.2567 4.2364-6.281 7.6657-11.457 18.43-12.932 26.891-1.4667 8.4111.71353 22.583 5.0764 32.996 3.8064 9.0852 13.569 25.149 22.801 37.517 1.3741 1.841 2.1708 2.9286 2.4712 3.5792 3.5437-1.1699 6.8496-1.9336 10.082-2.3263-1.3569-5.7831-4.6968-21.86-6.8361-33.002-.92884-4.8368-2.4692-14.322-3.2452-19.991-.68557-5.0083-.77707-6.9534-.74159-15.791.04316-10.803.41822-16.162 1.5026-21.503 1.4593-5.9026 3.3494-11.077 6.3247-15.852z"/>
     <path style="fill:currentColor" d="m94.499 885.78c-.10214-.0109-.13691 0-.0907.0409.16033.13489 1.329 1.0675 2.5976 2.0723 6.7003 5.307 11.273 14.568 12.658 25.638.52519 4.1949.24765 14.361-.5059 18.523-2.4775 13.684-9.7807 32.345-20.944 53.519l-3.0559 5.7971c2.8082.76579 5.7915 1.727 8.9926 2.8441 11.562-11.691 18.349-19.678 24.129-28.394 7.8992-11.913 11.132-20.234 12.24-31.518.98442-10.02-1.5579-20.876-6.7799-28.959-.2758-.4269-.57803-.86856-.89617-1.3166-3.247-6.13-9.752-12.053-21.264-16.131-2.3687-.86369-6.3657-2.0433-7.0802-2.1166z"/>
     <path style="fill:currentColor" d="m32.52 892.22c-.20090-.13016-1.4606.81389-3.9132 2.7457-11.486 9.0476-17.632 24.186-16.078 39.61.79699 7.9138 2.4066 13.505 5.9184 20.562 5.8577 11.77 14.749 23.219 30.087 38.74.05838.059.12188.1244.18052.1838 1.3166-.5556 2.5965-1.0618 3.8429-1.5199-.66408-.32448-1.4608-1.3297-3.8116-4.4602-5.0951-6.785-8.7512-11.962-13.051-18.486-5.1379-7.7948-5.0097-7.5894-8.0586-13.054-6.2097-11.13-8.2674-17.725-8.6014-27.563-.21552-6.3494.13041-9.2733 1.775-14.987 2.1832-7.5849 3.9273-10.986 9.2693-18.07 1.7839-2.3656 2.6418-3.57 2.4409-3.7003z"/>
     <path style="fill:currentColor" d="m69.133 992.37c-6.2405.0309-12.635.76718-19.554 2.5706 4.6956 4.7759 9.935 10.258 12.05 12.625l4.1272 4.6202h11.493l3.964-4.4516c2.0962-2.3541 7.4804-7.9845 12.201-12.768-8.378-1.4975-16.207-2.6353-24.281-2.5955z"/>
     <rect style="stroke-width:0;fill:currentColor" ry="2.0328" height="27.746" width="22.766" y="1017.7" x="60.201"/>
    </g>
   </g>
  </symbol>

  <symbol id="info" viewBox="0 0 41.667 41.667">
   <g transform="translate(-37.035 -1004.6)">
    <path style="stroke-linejoin:round;stroke:currentColor;stroke-linecap:round;stroke-width:3.728;fill:none" d="m76.25 1030.2a18.968 18.968 0 0 1 -23.037 13.709 18.968 18.968 0 0 1 -13.738 -23.019 18.968 18.968 0 0 1 23.001 -13.768 18.968 18.968 0 0 1 13.798 22.984"/>
    <g transform="matrix(1.1146 0 0 1.1146 -26.276 -124.92)">
     <path style="stroke:currentColor;stroke-linecap:round;stroke-width:3.728;fill:none" d="m75.491 1039.5v-8.7472"/>
     <path style="stroke-width:0;fill:currentColor" transform="scale(-1)" d="m-73.193-1024.5a2.3719 2.3719 0 0 1 -2.8807 1.7142 2.3719 2.3719 0 0 1 -1.718 -2.8785 2.3719 2.3719 0 0 1 2.8763 -1.7217 2.3719 2.3719 0 0 1 1.7254 2.8741"/>
    </g>
   </g>
  </symbol>

  <symbol id="warning" viewBox="0 0 48.430474 41.646302">
    <g transform="translate(-1.1273 -1010.2)">
     <path style="stroke-linejoin:round;stroke:currentColor;stroke-linecap:round;stroke-width:4.151;fill:none" d="m25.343 1012.3-22.14 37.496h44.28z"/>
     <path style="stroke:currentColor;stroke-linecap:round;stroke-width:4.1512;fill:none" d="m25.54 1027.7v8.7472"/>
     <path style="stroke-width:0;fill:currentColor" d="m27.839 1042.8a2.3719 2.3719 0 0 1 -2.8807 1.7143 2.3719 2.3719 0 0 1 -1.718 -2.8785 2.3719 2.3719 0 0 1 2.8763 -1.7217 2.3719 2.3719 0 0 1 1.7254 2.8741"/>
    </g>
  </symbol>

  <symbol id="menu" viewBox="0 0 50 50">
     <rect style="stroke-width:0;fill:currentColor" height="10" width="50" y="0" x="0"/>
     <rect style="stroke-width:0;fill:currentColor" height="10" width="50" y="20" x="0"/>
     <rect style="stroke-width:0;fill:currentColor" height="10" width="50" y="40" x="0"/>
   </symbol>

   <symbol id="link" viewBox="0 0 50 50">
    <g transform="translate(0 -1002.4)">
     <g transform="matrix(.095670 0 0 .095670 2.3233 1004.9)">
      <g>
       <path style="stroke-width:0;fill:currentColor" d="m452.84 192.9-128.65 128.65c-35.535 35.54-93.108 35.54-128.65 0l-42.881-42.886 42.881-42.876 42.884 42.876c11.845 11.822 31.064 11.846 42.886 0l128.64-128.64c11.816-11.831 11.816-31.066 0-42.9l-42.881-42.881c-11.822-11.814-31.064-11.814-42.887 0l-45.928 45.936c-21.292-12.531-45.491-17.905-69.449-16.291l72.501-72.526c35.535-35.521 93.136-35.521 128.64 0l42.886 42.881c35.535 35.523 35.535 93.141-.001 128.66zm-254.28 168.51-45.903 45.9c-11.845 11.846-31.064 11.817-42.881 0l-42.884-42.881c-11.845-11.821-11.845-31.041 0-42.886l128.65-128.65c11.819-11.814 31.069-11.814 42.884 0l42.886 42.886 42.876-42.886-42.876-42.881c-35.54-35.521-93.113-35.521-128.65 0l-128.65 128.64c-35.538 35.545-35.538 93.146 0 128.65l42.883 42.882c35.51 35.54 93.11 35.54 128.65 0l72.496-72.499c-23.956 1.597-48.092-3.784-69.474-16.283z"/>
      </g>
     </g>
    </g>
  </symbol>

  <symbol id="doc" viewBox="0 0 35 45">
   <g transform="translate(-147.53 -539.83)">
    <path style="stroke:currentColor;stroke-width:2.4501;fill:none" d="m149.38 542.67v39.194h31.354v-39.194z"/>
    <g style="stroke-width:25" transform="matrix(.098003 0 0 .098003 133.69 525.96)">
     <path d="m220 252.36h200" style="stroke:currentColor;stroke-width:25;fill:none"/>
     <path style="stroke:currentColor;stroke-width:25;fill:none" d="m220 409.95h200"/>
     <path d="m220 488.74h200" style="stroke:currentColor;stroke-width:25;fill:none"/>
     <path d="m220 331.15h200" style="stroke:currentColor;stroke-width:25;fill:none"/>
    </g>
   </g>
 </symbol>

 <symbol id="tick" viewBox="0 0 177.16535 177.16535">
  <g transform="translate(0 -875.2)">
   <rect style="stroke-width:0;fill:currentColor" transform="rotate(30)" height="155" width="40" y="702.99" x="556.82"/>
   <rect style="stroke-width:0;fill:currentColor" transform="rotate(30)" height="40" width="90.404" y="817.99" x="506.42"/>
  </g>
 </symbol>
</svg>

    <div class="wrapper">
      <header class="intro-and-nav" role="banner">
  <div>
    <div class="intro">
      <a
        class="logo"
        href="https://example.com"
        aria-label="Cupper home page"
      >
        <img src="https://example.com/images/logo.svg" alt="">
      </a>
      <p class="library-desc">
         Data science ramblings and notes 
      </p>
    </div>
    <nav id="patterns-nav" class="patterns" role="navigation">
  <h2 class="vh">Main navigation</h2>
  <button id="menu-button" aria-expanded="false">
    <svg viewBox="0 0 50 50" aria-hidden="true" focusable="false">
      <use xlink:href="#menu"></use>
    </svg>
    Menu
  </button>
  
  <ul id="patterns-list">
  
    <li class="pattern">
      
      
      
      
      <a href="/" >
        <svg class="bookmark-icon" aria-hidden="true" focusable="false" viewBox="0 0 40 50">
          <use xlink:href="#bookmark"></use>
        </svg>
        <span class="text">Home</span>
      </a>
    </li>
  
    <li class="pattern">
      
      
      
      
      <a href="/post/" aria-current="page">
        <svg class="bookmark-icon" aria-hidden="true" focusable="false" viewBox="0 0 40 50">
          <use xlink:href="#bookmark"></use>
        </svg>
        <span class="text">Blog</span>
      </a>
    </li>
  
    <li class="pattern">
      
      
      
      
      <a href="/tags/" >
        <svg class="bookmark-icon" aria-hidden="true" focusable="false" viewBox="0 0 40 50">
          <use xlink:href="#bookmark"></use>
        </svg>
        <span class="text">Tags</span>
      </a>
    </li>
  
    <li class="pattern">
      
      
      
      
      <a href="/about/" >
        <svg class="bookmark-icon" aria-hidden="true" focusable="false" viewBox="0 0 40 50">
          <use xlink:href="#bookmark"></use>
        </svg>
        <span class="text">About</span>
      </a>
    </li>
  
    <li class="pattern">
      
      
      
      
      <a href="/index.xml" >
        <svg class="bookmark-icon" aria-hidden="true" focusable="false" viewBox="0 0 40 50">
          <use xlink:href="#bookmark"></use>
        </svg>
        <span class="text">RSS</span>
      </a>
    </li>
  
  </ul>
</nav>
  </div>
</header>

      <div class="main-and-footer">
        <div>
          
  <main id="main">
    <h1>
      <svg class="bookmark-icon" aria-hidden="true" viewBox="0 0 40 50" focusable="false">
        <use xlink:href="#bookmark"></use>
      </svg>
      SVMs - Part II (Application)
    </h1>

    <div class="date">
      
      
      <strong>Publish date: </strong>May 2, 2020
      
        
      
    </div>

    
      <div class="tags">
        <strong>Tags: </strong>
        <ul aria-label="tags">
          
            <li>
              <svg class="tag-icon" aria-hidden="true" viewBox="0 0 177.16535 177.16535" focusable="false">
                <use xlink:href="#tag"></use>
              </svg>
              
              <a href="https://example.com/tags/machine-learning/">machine-learning</a>
            </li>
          
            <li>
              <svg class="tag-icon" aria-hidden="true" viewBox="0 0 177.16535 177.16535" focusable="false">
                <use xlink:href="#tag"></use>
              </svg>
              
              <a href="https://example.com/tags/svm/">svm</a>
            </li>
          
            <li>
              <svg class="tag-icon" aria-hidden="true" viewBox="0 0 177.16535 177.16535" focusable="false">
                <use xlink:href="#tag"></use>
              </svg>
              
              <a href="https://example.com/tags/optimization/">optimization</a>
            </li>
          
        </ul>
      </div>
    
    
    
      


    

    <p>Now that we have the theory of Support Vector Machines, I am now going to code a Support Vector Machine from Scratch. First, I am going to start with some data.</p>
<pre><code class="language-python">import cvxpy as cp
import pandas as pd 
import matplotlib.pyplot as plt 
from sklearn.datasets import make_blobs
import numpy as np
</code></pre>
<pre><code class="language-python"># create dataset
data, labels = make_blobs(n_features=2, centers=2, cluster_std=1.25, 
                          random_state=11, n_samples = 10)
df = pd.DataFrame({'Credit Score': data[:, 0], 'Income': data[:, 1], 
                   'Target': labels})

with plt.style.context('fivethirtyeight'):
    fig, ax = plt.subplots(figsize = (8, 8))
    df[df['Target'] == 0].plot(x = 'Credit Score', y = 'Income', color = 'red', 
                              kind = 'scatter', ax = ax,
                              s = 100, label = 'default')
    df[df['Target'] == 1].plot(x = 'Credit Score', y = 'Income', color = 'blue', 
                              kind = 'scatter', ax = ax,
                              s = 100, label = 'repaid')
</code></pre>
<p><img src="https://raw.githubusercontent.com/sik-flow/sik-flow.github.io/master/_posts/Images/SVMs_2_files/SVMs_2_2_0.png" alt="png"></p>
<p>I am going to predict whether someone repays or defaults on their loan based on their credit score and income.  In this instance, the data has been scaled. We see with this problem I can use a hard classifier because there is a clear seperation between the 2 classes.</p>
<p>I am going to convert the target variable to be 1 and -1 so that I can utilize this formula:<br> $$(a_1x_{1j} + a_2x_{2j} + &hellip; + a_mx_{mj} + a_0)y_j \geq 1$$</p>
<pre><code class="language-python">df['Target'] = df['Target'].map(lambda x: -1 if x == 0 else x)
df
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Credit Score</th>
      <th>Income</th>
      <th>Target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>-0.774473</td>
      <td>3.644370</td>
      <td>1</td>
    </tr>
    <tr>
      <td>1</td>
      <td>-7.502406</td>
      <td>-10.205162</td>
      <td>-1</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.633908</td>
      <td>4.111708</td>
      <td>1</td>
    </tr>
    <tr>
      <td>3</td>
      <td>-7.000313</td>
      <td>-12.927143</td>
      <td>-1</td>
    </tr>
    <tr>
      <td>4</td>
      <td>-6.404962</td>
      <td>-10.010034</td>
      <td>-1</td>
    </tr>
    <tr>
      <td>5</td>
      <td>-2.367565</td>
      <td>3.099335</td>
      <td>1</td>
    </tr>
    <tr>
      <td>6</td>
      <td>-5.868293</td>
      <td>-10.942499</td>
      <td>-1</td>
    </tr>
    <tr>
      <td>7</td>
      <td>-7.065393</td>
      <td>-9.216242</td>
      <td>-1</td>
    </tr>
    <tr>
      <td>8</td>
      <td>0.185417</td>
      <td>6.466971</td>
      <td>1</td>
    </tr>
    <tr>
      <td>9</td>
      <td>0.126473</td>
      <td>5.200169</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>
<p>Solving for a support vector machine is a classic optimization problem.  An optimization problem consists of 3 parts - variables, constraints, and objective function.  For this specific problem they are as follows:</p>
<ol>
<li>Variables: I am looking for values for each of the coefficients (Credit Score and Income) and a value for the intercept  ($$a_1$$, $$a_2$$, and $$a_0$$)</li>
<li>Constraints: Since I am going to be using a hard classifier I need to find values for each coefficient that allows for each point to be on the correct side of the classifier ($$(a_1x_{1j} + a_2x_{2j} + &hellip; + a_mx_{mj} + a_0)y_j \geq 1$$)</li>
<li>Objective Function: I want to maximize the margin of the classifier.  To do this I need to minimize $$\sum_i(a_i)^2$$</li>
</ol>
<p>I am now going to code this out using the CVXPY library used for solving convex optimization problems.</p>
<pre><code class="language-python"># Make variables
a0 = cp.Variable()
a1 = cp.Variable()
a2 = cp.Variable()
</code></pre>
<pre><code class="language-python"># Make constraints
constraints = []
for index, row in df.iterrows():
    constraints.append(((row['Credit Score'] * a1) + \ 
                (row['Income']) * a2 + a0) * row['Target'] &gt;= 1)
</code></pre>
<pre><code class="language-python"># Make objective function
obj = cp.Minimize((a1**2) + (a2**2))
</code></pre>
<pre><code class="language-python">svm_prob = cp.Problem(obj, constraints)
</code></pre>
<pre><code class="language-python">svm_prob.solve() 
svm_prob.status
</code></pre>
<pre><code>'optimal'
</code></pre>
<p>This means that our optimizer successfully found variables ($$a_1, a_2, a_0)$$ that met all of our constraints.  Now I will take a look at what these variables are.</p>
<pre><code class="language-python">a0.value, a1.value, a2.value
</code></pre>
<pre><code>(array(0.6886473), array(0.05407789), array(0.14176774))
</code></pre>
<p>I am now going to plot my classifier.  To do this I know the equation of the line is $$Credit Score * a_1 + Income * a_2 + a_0 = 0$$.  To solve for income the equation becomes<br> $$Income = \dfrac{-Credit Score * a_1 - a_0}{a_2}$$</p>
<pre><code class="language-python">vals = []
for index, row in df.iterrows():
    vals.append((((-row['Credit Score'] * a1.value) - a0.value) / a2.value))
with plt.style.context('fivethirtyeight'):
    fig, ax = plt.subplots(figsize = (8, 8))
    df[df['Target'] == -1].plot(x = 'Credit Score', y = 'Income', color = 'red',
                              kind = 'scatter', ax = ax,
                              s = 100, label = 'default')
    df[df['Target'] == 1].plot(x = 'Credit Score', y = 'Income', color = 'blue',  
                              kind = 'scatter', ax = ax,
                              s = 100, label = 'repaid')
    plt.plot(df['Credit Score'], vals, color = 'k', label = 'Classifier')
    plt.legend()
</code></pre>
<p><img src="https://raw.githubusercontent.com/sik-flow/sik-flow.github.io/master/_posts/Images/SVMs_2_files/SVMs_2_14_0.png" alt="png"></p>
<p>We see the classifier in relation to the two categories and see that it does a good job of classifying the two categories.  Now I want to find the support vectors. The support vector for the positive class is<br>
$$Credit Score * a_1 + Income * a_2 + a_0 = 1$$<br>
The support vector for the negative class is <br>
$$Credit Score * a_1 + Income * a_2 + a_0 = -1$$</p>
<pre><code class="language-python">vals = []
upper_support = []
lower_support = []
for index, row in df.iterrows():
    vals.append((((-row['Credit Score'] * a1.value) - a0.value ) / a2.value))
    upper_support.append((((-row['Credit Score'] * a1.value) - \
                  a0.value + 1) / a2.value))
    lower_support.append((((-row['Credit Score'] * a1.value) - \
                  a0.value - 1) / a2.value))
with plt.style.context('fivethirtyeight'):
    fig, ax = plt.subplots(figsize = (8, 8))
    df[df['Target'] == -1].plot(x = 'Credit Score', y = 'Income', color = 'red',
                              kind = 'scatter', ax = ax,
                              s = 100, label = 'default')
    df[df['Target'] == 1].plot(x = 'Credit Score', y = 'Income', color = 'blue', 
                              kind = 'scatter', ax = ax,
                              s = 100, label = 'repaid')
    plt.plot(df['Credit Score'], vals, color = 'k')
    plt.plot(df['Credit Score'], upper_support, color = 'green')
    plt.plot(df['Credit Score'], lower_support, color = 'green')
</code></pre>
<p><img src="https://raw.githubusercontent.com/sik-flow/sik-flow.github.io/master/_posts/Images/SVMs_2_files/correct_image.png" alt="png"></p>
<p>Remaking the graph with the support vectors we can see the margin between the 2 classifiers. The optimization problem successfully found the line that would maximize the margin between the 2 classes.  I want to see how my model does versus the implementation of <a href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html">sklearn&rsquo;s support vector classifier</a>.</p>
<pre><code class="language-python">from sklearn.svm import SVC
svm = SVC(kernel = 'linear', C = 1E100)
</code></pre>
<p>I am using a linear kernel and a very large <code>C</code> value.  This is because I used a linear classifier and I used a hard classifier, as <code>C</code> increases the importance of finding as many values correct outweighs the importance of finding a classifier that maximizes the margin.</p>
<pre><code class="language-python">svm.fit(df.drop('Target', axis = 1), df['Target'])
</code></pre>
<pre><code>SVC(C=1e+100, cache_size=200, class_weight=None, coef0=0.0,
    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',
    kernel='linear', max_iter=-1, probability=False, random_state=None,
    shrinking=True, tol=0.001, verbose=False)
</code></pre>
<p>Model has been fit.  Lets see what the coefficients and intercept are</p>
<pre><code class="language-python">svm.coef_, svm.intercept_
</code></pre>
<pre><code>(array([[0.05407789, 0.14176774]]), array([0.68864728]))
</code></pre>
<pre><code class="language-python"># my coefficeints and intercepts
a1.value, a2.value, a0.value
</code></pre>
<pre><code>(array(0.05407789), array(0.14176774), array(0.6886473))
</code></pre>
<p>We see that they are almost exactly the same number!</p>
<h4 id="interpretting-the-results">Interpretting the Results</h4>
<p>A big part of machine learning is making sure that you understand your model.  What do the values of the coefficients mean? Credit Score has a coefficient of 0.05 and income has a coefficient of 0.14.  Income&rsquo;s coefficient is almost 3 times larger than credit score&rsquo;s coefficient.  This implies that income is a more important predictor of whether someone will repay a loan than credit score (remember this is randomly generated data).  Lets explore this.</p>
<pre><code class="language-python">diff_credit_score = df[df['Target'] == 1]['Credit Score'].median() - \
                    df[df['Target'] != 1]['Credit Score'].median()
diff_income = df[df['Target'] == 1]['Income'].median() - \
              df[df['Target'] != 1]['Income'].median()

with plt.style.context('fivethirtyeight'):
    fig, ax = plt.subplots(figsize = (8, 8))
    ax.bar(['Credit Score', 'Income'], [diff_credit_score, diff_income])
    ax.set_title('Median Difference Between Income and Credit Score')
    ax.set_ylabel('Median Difference')
    ax.text(-.1, 6, np.round(diff_credit_score, 2), fontsize = 18)
    ax.text(.9, 13, np.round(diff_income, 2))
</code></pre>
<p><img src="https://raw.githubusercontent.com/sik-flow/sik-flow.github.io/master/_posts/Images/SVMs_2_files/SVMs_2_25_0.png" alt="png"></p>
<p>We see that the median difference between the two classes for credit score is 7 and the median difference between the two classes for income is 14.  So, the difference is much larger in income and the coefficent also supports that income is a more important variable than credit score.</p>
<p>Now lets try out a dataset that a linear classifier would not be useful.</p>
<pre><code class="language-python">data, labels = make_blobs(n_features=2, centers=2, cluster_std=11,  
                          random_state=11, n_samples = 10)
df = pd.DataFrame({'Credit Score': data[:, 0], 'Income': data[:, 1], 
                  'Target': labels})

with plt.style.context('fivethirtyeight'):
    fig, ax = plt.subplots(figsize = (8, 8))
    df[df['Target'] == 0].plot(x = 'Credit Score', y = 'Income', color = 'red', 
                              kind = 'scatter', ax = ax,
                              s = 100, label = 'default')
    df[df['Target'] == 1].plot(x = 'Credit Score', y = 'Income', color = 'blue', 
                              kind = 'scatter', ax = ax,
                              s = 100, label = 'repaid')
</code></pre>
<p><img src="https://raw.githubusercontent.com/sik-flow/sik-flow.github.io/master/_posts/Images/SVMs_2_files/SVMs_2_27_0.png" alt="png"></p>
<pre><code class="language-python"># Make sure target variable is 1 and -1 
df['Target'] = df['Target'].map(lambda x: -1 if x == 0 else x)
df
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Credit Score</th>
      <th>Income</th>
      <th>Target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>-1.077455</td>
      <td>-3.019234</td>
      <td>1</td>
    </tr>
    <tr>
      <td>1</td>
      <td>-16.143243</td>
      <td>-14.843564</td>
      <td>-1</td>
    </tr>
    <tr>
      <td>2</td>
      <td>11.316297</td>
      <td>1.093336</td>
      <td>1</td>
    </tr>
    <tr>
      <td>3</td>
      <td>-11.724823</td>
      <td>-38.796999</td>
      <td>-1</td>
    </tr>
    <tr>
      <td>4</td>
      <td>-6.485737</td>
      <td>-13.126440</td>
      <td>-1</td>
    </tr>
    <tr>
      <td>5</td>
      <td>-15.096663</td>
      <td>-7.815549</td>
      <td>1</td>
    </tr>
    <tr>
      <td>6</td>
      <td>-1.763048</td>
      <td>-21.332128</td>
      <td>-1</td>
    </tr>
    <tr>
      <td>7</td>
      <td>-12.297529</td>
      <td>-6.141066</td>
      <td>-1</td>
    </tr>
    <tr>
      <td>8</td>
      <td>7.369582</td>
      <td>21.819653</td>
      <td>1</td>
    </tr>
    <tr>
      <td>9</td>
      <td>6.850876</td>
      <td>10.671793</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>
<p>We see that we will not be able to fit a straight line through this dataset and have all the points will be on the correct side of the line. We will not be able to find a line that would meet our constraint of $$(a_1x_{1j} + a_2x_{2j} + &hellip; + a_mx_{mj} + a_0)y_j \geq 1$$
With that being said, let&rsquo;s see what happens if we try.</p>
<pre><code class="language-python"># Make variables
a0 = cp.Variable()
a1 = cp.Variable()
a2 = cp.Variable()
</code></pre>
<pre><code class="language-python"># Make constraints
constraints = []
for index, row in df.iterrows():
    constraints.append(((row['Credit Score'] * a1) + \
    (row['Income']) * a2 + a0) * row['Target'] &gt;= 1)
</code></pre>
<pre><code class="language-python"># Make objective function
obj = cp.Minimize((a1**2) + (a2**2))
</code></pre>
<pre><code class="language-python">svm_prob_2 = cp.Problem(obj, constraints)
svm_prob_2.solve() 
svm_prob_2.status
</code></pre>
<pre><code>'infeasible'
</code></pre>
<p>We see that the results of this is <code>infeasible</code>, which is to be expected.  Now I am going to try a soft classifer, where I am all right with a few misclassifications assuming I can maximize the margin.</p>
<h4 id="soft-classifier">Soft Classifier</h4>
<p>For a soft classifier we will need to change our variables, constraints, and objective function of the classifier.  Let&rsquo;s start with the constraint:
$$(a_1x_{1j} + a_2x_{2j} + &hellip; + a_mx_{mj} + a_0)y_j \geq 1 - \xi$$</p>
<p>This formula has a new variable $$\xi$$.  If the point is on the correct side of the line then, $$\xi$$ will be 0, however if it is on the wrong side of the line then there will be a value for $$\xi$$ so that the constraint is still accurate.  The larger the value for $$\xi$$ the larger the error.  This then leads to the objective function:
Minimize $$\sum_i(a_i)^2 + C(\sum_i \xi^{(i)})$$
<code>C</code> is a value to specify to trade off between maximizing the margin and minimizing mistakes.  A low value for <code>C</code> means that our classifier is prioritizing a large margin and a large value for <code>C</code> means that our classifier is prioritizing minimizing the number of mistakes.</p>
<p>I&rsquo;m now going to walk through all 3 parts of my optimization function starting with the variables. I will need the following variables:</p>
<ul>
<li>$$a_1$$: coefficient for credit score</li>
<li>$$a_2$$: coefficient for income</li>
<li>$$a_0$$: intercept</li>
<li>$$\xi$$: amount each point is off (note there will be a $$\xi$$ for each data point).</li>
</ul>
<p>Coding this will be as follows:</p>
<pre><code class="language-python"># variables 
a0 = cp.Variable()
a1 = cp.Variable()
a2 = cp.Variable()
ksi = cp.Variable(10)
</code></pre>
<p>I have the following constraints:<br>
$$(a_1x_{1j} + a_2x_{2j} + &hellip; + a_mx_{mj} + a_0)y_j \geq 1 - \xi$$<br>
$$\xi \geq 0$$</p>
<p>I need $$\xi$$ to be greater than or equal to be zero.  I have to specify this or the optimizer will &ldquo;cheat&rdquo; and make some of the $$\xi$$ variable negative.  Coding this will be as follows:</p>
<pre><code class="language-python"># the constraints 
constraints_coef = []
for index, row in df.iterrows():
    constraints_coef.append(((row['Credit Score'] * a1) + \
                             (row['Income']) * a2 + a0) * row['Target'] &gt;= 1 \
                                                                - ksi[index])
ksi_constraints = [ksi &gt;= 0 for x_i in range(100)]

constraints = constraints_coef + ksi_constraints
</code></pre>
<p>Finally, like I mentioned earlier my objective function is:<br>
Minimize $$\sum_i(a_i)^2 + C(\sum_i \xi^{(i)})$$
To code that is as follows (going to specify <code>C</code> = 1):</p>
<pre><code class="language-python">C = 1
soft_objective = cp.Minimize(((a1**2) + (a2**2)) + C*(sum(ksi)))
</code></pre>
<pre><code class="language-python">soft_classifier = cp.Problem(soft_objective, constraints)
soft_classifier.solve(solver = 'ECOS')
soft_classifier.status
</code></pre>
<pre><code>'optimal'
</code></pre>
<p>I now get an optimal result.  Lets see how the graph looks.</p>
<pre><code class="language-python">vals = []
upper_support = []
lower_support = []
for index, row in df.iterrows():
    vals.append((((-row['Credit Score'] * a1.value) - a0.value) / a2.value))
    upper_support.append((((-row['Credit Score'] * a1.value) - \
                            a0.value + 1) / a2.value))
    lower_support.append((((-row['Credit Score'] * a1.value) - \
                            a0.value - 1) / a2.value))

with plt.style.context('fivethirtyeight'):
    fig, ax = plt.subplots(figsize = (8, 8))
    df[df['Target'] == -1].plot(x = 'Credit Score', y = 'Income', color = 'red', 
                              kind = 'scatter', ax = ax,
                              s = 100, label = 'default')
    df[df['Target'] == 1].plot(x = 'Credit Score', y = 'Income', color = 'blue', 
                              kind = 'scatter', ax = ax,
                              s = 100, label = 'repaid')
    plt.plot(df['Credit Score'], vals, color = 'k')
    plt.plot(df['Credit Score'], upper_support, color = 'green')
    plt.plot(df['Credit Score'], lower_support, color = 'green')
</code></pre>
<p><img src="https://raw.githubusercontent.com/sik-flow/sik-flow.github.io/master/_posts/Images/SVMs_2_files/SVMs_2_45_0.png" alt="png"></p>
<p>We now see that we predict one to repay their loan that acutally defaulted on their loan.  I am going to turn this into a function, so that we can try out different values of C and see how the plot changes.</p>
<pre><code class="language-python">data, labels = make_blobs(n_features=2, centers=2, cluster_std=11, 
                         random_state=11, n_samples = 10)
df = pd.DataFrame({'Credit Score': data[:, 0], 'Income': data[:, 1], 
                          'Target': labels})
</code></pre>
<pre><code class="language-python">def soft_margin_svm(C):
    '''
    Function to solve for a soft margin support vector machine classifier
    
    Args
        C (float): Tradeoff between maximizing margin and minimizing mistakes.  
                   Small value of C will maximize the margin and a large 
                   value of C will minimize the mistakes
                   
    Returns
        optimization model and plot showing classifier and support vectors
    '''
    data, labels = make_blobs(n_features=2, centers=2, cluster_std=11,  
                              random_state=11, n_samples = 10)
    df = pd.DataFrame({'Credit Score': data[:, 0], 'Income': data[:, 1],
                       'Target': labels})
    df['Target'] = df['Target'].map(lambda x: -1 if x == 0 else x)

    # variables
    a0 = cp.Variable()
    a1 = cp.Variable()
    a2 = cp.Variable()
    ksi = cp.Variable(10)
    
    # the constraints 
    constraints_coef = []
    for index, row in df.iterrows():
        constraints_coef.append(((row['Credit Score'] * a1) + \
                                 (row['Income']) * a2 + a0) * \ 
                                 row['Target'] &gt;= 1 - ksi[index])
    ksi_constraints = [ksi &gt;= 0 for x_i in range(100)]

    constraints = constraints_coef + ksi_constraints
    
    # objective function 
    soft_objective = cp.Minimize(((a1**2) + (a2**2)) + C*(sum(ksi)))
    
    soft_classifier = cp.Problem(soft_objective, constraints)
    soft_classifier.solve(solver = 'ECOS')
    
    # make plot
    data, labels = make_blobs(n_features=2, centers=2, cluster_std=11, 
                               random_state=11, n_samples = 10)
    df = pd.DataFrame({'Credit Score': data[:, 0], 'Income': data[:, 1], 
                      'Target': labels})
    vals = []
    upper_support = []
    lower_support = []
    for index, row in df.iterrows():
        vals.append((((-row['Credit Score'] * a1.value) - a0.value) / a2.value))
        upper_support.append((((-row['Credit Score'] * a1.value) - \
                                a0.value + 1) / a2.value))
        lower_support.append((((-row['Credit Score'] * a1.value) - \ 
                                a0.value - 1) / a2.value))

    with plt.style.context('fivethirtyeight'):
        fig, ax = plt.subplots(figsize = (8, 8))
        df[df['Target'] == 0].plot(x = 'Credit Score', y = 'Income', 
                                  color = 'red', 
                                  kind = 'scatter', ax = ax,
                                  s = 100, label = 'default')
        df[df['Target'] == 1].plot(x = 'Credit Score', y = 'Income', 
                                  color = 'blue', 
                                  kind = 'scatter', ax = ax,
                                  s = 100, label = 'repaid')
        plt.plot(df['Credit Score'], vals, color = 'k')
        plt.plot(df['Credit Score'], upper_support, color = 'green')
        plt.plot(df['Credit Score'], lower_support, color = 'green')
        plt.title(f'C = {C}')
        
    return soft_classifier
</code></pre>
<pre><code class="language-python">soft_margin_svm(.001);
</code></pre>
<p><img src="https://raw.githubusercontent.com/sik-flow/sik-flow.github.io/master/_posts/Images/SVMs_2_files/SVMs_2_49_0.png" alt="png"></p>
<pre><code class="language-python">soft_margin_svm(1);
</code></pre>
<p><img src="https://raw.githubusercontent.com/sik-flow/sik-flow.github.io/master/_posts/Images/SVMs_2_files/SVMs_2_50_0.png" alt="png"></p>
<pre><code class="language-python">soft_margin_svm(1000);
</code></pre>
<p><img src="https://raw.githubusercontent.com/sik-flow/sik-flow.github.io/master/_posts/Images/SVMs_2_files/SVMs_2_51_0.png" alt="png"></p>
<p>We see that as <code>C</code> gets larger the margin gets smaller. One more thing I want to cover is what if predicting one class is more important than predicting the other class.  For example, lets say that I am very conservative and only want to give loans to people that I am very confident will repay their loans.  I can change the equation of the line so that I move it away from the default class and towards the class I want to be conservative towards.  To do this I change the equation of the line to be the following:</p>
<p>$$a_1 * credit score + a_2 * income + [\dfrac{2}{3}(a_0 + 1) + \dfrac{1}{3}(a_0 - 1)] = 0$$
Which becomes:
$$a_1 * credit score + a_2 * income + [a_0 + \dfrac{1}{3}] = 0$$</p>
<p>This implies that predicting class 1 (predicting they repaid and they don&rsquo;t) is twice as costly as predicting class 1 (predicting they don&rsquo;t repay and they do repay).  In essence, giving a bad loan is twice as costly as withholding a good loan. Let&rsquo;s see how this changes our line.</p>
<pre><code class="language-python">vals = []
upper_support = []
lower_support = []
classifier_unequal = []
upper_unequal = []
lower_unequal = []
for index, row in df.iterrows():
    vals.append((((-row['Credit Score'] * a1.value) - a0.value) / a2.value))
    upper_support.append((((-row['Credit Score'] * a1.value) - \
                          a0.value + 1) / a2.value))
    lower_support.append((((-row['Credit Score'] * a1.value) - \
                          a0.value - 1) / a2.value))
    classifier_unequal.append((((-row['Credit Score'] * a1.value) - \
                          a0.value + (1/3)) / a2.value))
    upper_unequal.append((((-row['Credit Score'] * a1.value) - \
                          a0.value + (4/3)) / a2.value))
    lower_unequal.append((((-row['Credit Score'] * a1.value) - \
                          a0.value - (4/3)) / a2.value))

with plt.style.context('fivethirtyeight'):
    fig = plt.figure(figsize = (12, 8))
    ax1 = plt.subplot(1, 2, 1)
    df[df['Target'] == -1].plot(x = 'Credit Score', y = 'Income', color = 'red', 
                              kind = 'scatter', ax = ax1,
                              s = 100, label = 'default')
    df[df['Target'] == 1].plot(x = 'Credit Score', y = 'Income', color = 'blue', 
                              kind = 'scatter', ax = ax1,
                              s = 100, label = 'repaid')
    plt.plot(df['Credit Score'], vals, color = 'k')
    plt.plot(df['Credit Score'], upper_support, color = 'green')
    plt.plot(df['Credit Score'], lower_support, color = 'green')
    plt.legend(loc = 'upper left')
    plt.title('Each class is equal')
    
    ax2 = plt.subplot(1, 2, 2)
    df[df['Target'] == -1].plot(x = 'Credit Score', y = 'Income', color = 'red', 
                              kind = 'scatter', ax = ax2,
                              s = 100, label = 'default')
    df[df['Target'] == 1].plot(x = 'Credit Score', y = 'Income', color = 'blue', 
                              kind = 'scatter', ax = ax2,
                              s = 100, label = 'repaid')
    plt.plot(df['Credit Score'], classifier_unequal, color = 'k')
    plt.plot(df['Credit Score'], upper_unequal, color = 'green')
    plt.plot(df['Credit Score'], lower_unequal, color = 'green')
    plt.legend(loc = 'upper left')
    plt.title('Classes are unequal')
</code></pre>
<p><img src="https://raw.githubusercontent.com/sik-flow/sik-flow.github.io/master/_posts/Images/SVMs_2_files/SVMs_2_53_0.png" alt="png"></p>
<p>We can see the plot on the right, that the classifier is closer to the repaid.  This will cause less values to be classified as repaid and limit the risk that the company is taking.  This comes with the sacrifice of not giving some people loans that may repay their loans.</p>

  </main>
  <div id="disqus-container">
  
    <button id="disqus-button" onclick="showComments()">Show comments</button>
    <div id="disqus-comments">
      
      
      
        <p><em>Disqus comments are disabled.</em></p>
        <script type="application/javascript">
          function showComments() {
            // Remove button
var disqusButton = document.getElementById('disqus-button');
disqusButton.parentNode.removeChild(disqusButton); 
// Un-hide comments
var disqusComments = document.getElementById('disqus-comments');
disqusComments.style.display = 'block'; 
          }
        </script>
      
      <noscript>Enable JavaScript to view Disqus comments.</noscript>
    </div>
  
</div>


          <footer role="contentinfo">
  <div>
    <label for="themer">
      dark theme: <input type="checkbox" id="themer" class="vh">
      <span aria-hidden="true"></span>
    </label>
  </div>
  
    Follow me on <a href="https://www.linkedin.com/in/jeff-herman">Linkedin</a>
  
</footer>

        </div>
      </div>
    </div>
    <script src="https://example.com/js/prism.js"></script>



<script src="https://example.com/js/dom-scripts.js"></script>



<script src="/js/search.7aef046a0cc8b0c532f1d20087b920459bc868c936bb48a6ae221eceefca2d07.js"></script>

<link rel="stylesheet" href="/css/search.fe0cd54a21628574bff49d721c827d1bb165ab56b0f22dd55ae78addbe61c309.css"></link>


    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/katex.min.css" integrity="sha384-dbVIfZGuN1Yq7/1Ocstc1lUEm+AT+/rCkibIcC/OmWo5f0EA48Vf8CytHzGrSwbQ" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/katex.min.js" integrity="sha384-2BKqo+exmr9su6dir+qCw08N2ZKRucY4PrGQPPWU1A7FtlCGjmEGFqXCv5nyM5Ij" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>

    
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-123456789-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  </body>
</html>
